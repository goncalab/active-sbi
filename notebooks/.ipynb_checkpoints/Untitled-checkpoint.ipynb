{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d7100-75ad-4eb4-ae7e-21ecaf5c137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████████████████████████████████████████▉                                                                                                             | 38/100 [00:12<00:20,  3.05it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Simulator and True Likelihood Definitions ------------------------------\n",
    "\n",
    "def f_1d(theta):\n",
    "    \"\"\"\n",
    "    Cubic nonlinearity, f(theta) = ((1.5*theta + 0.5)**3) / 200.\n",
    "    theta can be a numpy array.\n",
    "    \"\"\"\n",
    "    return ((1.5 * theta + 0.5) ** 3) / 200\n",
    "\n",
    "def simulator_1d(theta, sigma=0.31622777):\n",
    "    \"\"\"Generate a simulated observation x ~ N(f(theta), sigma^2).\"\"\"\n",
    "    mean = f_1d(theta)\n",
    "    return np.random.normal(mean, sigma)\n",
    "\n",
    "def true_log_likelihood_1d(theta, x0=2, sigma=0.31622777):\n",
    "    \"\"\"\n",
    "    Compute the true log-likelihood: log p(x0|theta) = log N(x0; f(theta), sigma^2)\n",
    "    \"\"\"\n",
    "    mean = f_1d(theta)\n",
    "    return -0.5*np.log(2*np.pi*sigma**2) - 0.5*((x0 - mean)**2)/(sigma**2)\n",
    "\n",
    "def true_posterior_1d(theta_grid, x0=2, sigma=0.31622777):\n",
    "    \"\"\"Compute the (normalized) true posterior on a grid (prior is uniform).\"\"\"\n",
    "    ll = np.exp(true_log_likelihood_1d(theta_grid, x0, sigma))\n",
    "    post = ll  # uniform prior constant => p(theta|x0) ∝ likelihood\n",
    "    norm = np.trapz(post, theta_grid) + 1e-10\n",
    "    return post / norm\n",
    "\n",
    "# --- Neural Network Ensemble Definition ---------------------------------------\n",
    "\n",
    "class EmulatorNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward network that predicts the parameters of a Gaussian distribution.\n",
    "    Input: theta (1D)\n",
    "    Output: [mu, log_sigma] where sigma = exp(log_sigma)\n",
    "    Architecture: one hidden layer with 10 tanh units.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(EmulatorNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.act = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(10, 2)  # outputs: mu and log_sigma\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is expected to be of shape (batch, 1)\n",
    "        h = self.act(self.fc1(x))\n",
    "        out = self.fc2(h)\n",
    "        return out  # shape: (batch, 2)\n",
    "\n",
    "def nll_loss(output, target):\n",
    "    \"\"\"\n",
    "    Negative log likelihood loss for a Gaussian likelihood.\n",
    "    output: tensor of shape (batch, 2) -> [mu, log_sigma]\n",
    "    target: tensor of shape (batch, 1)\n",
    "    \"\"\"\n",
    "    mu = output[:, 0].unsqueeze(1)\n",
    "    log_sigma = output[:, 1].unsqueeze(1)\n",
    "    sigma = torch.exp(log_sigma) + 1e-6  # ensure positivity\n",
    "    # Using the closed-form Gaussian NLL loss:\n",
    "    loss = 0.5 * torch.log(2*np.pi*sigma**2) + 0.5 * ((target - mu)**2)/(sigma**2)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# --- Training Functions for the Ensemble ------------------------------------\n",
    "\n",
    "def train_ensemble(models, optimizers, theta_tensor, x_tensor, epochs=200):\n",
    "    \"\"\"\n",
    "    Train each ensemble member on the current dataset.\n",
    "    models: list of EmulatorNet instances.\n",
    "    optimizers: list of optimizers (one per model).\n",
    "    theta_tensor: tensor of shape (N, 1)\n",
    "    x_tensor: tensor of shape (N, 1)\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        for model, optimizer in zip(models, optimizers):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(theta_tensor)\n",
    "            loss = nll_loss(outputs, x_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def ensemble_predict(models, theta_candidates, x0=2, sigma_min=1e-6):\n",
    "    \"\"\"\n",
    "    For a set of candidate theta values, compute the synthetic likelihood q(x0|theta)\n",
    "    for each ensemble member and return an array of shape (n_candidates, n_models).\n",
    "    The likelihood for a given network is computed as:\n",
    "        L = N(x0; mu(theta), sigma(theta)^2)\n",
    "    \"\"\"\n",
    "    models_eval = [model.eval() for model in models]\n",
    "    theta_tensor = torch.tensor(theta_candidates.reshape(-1, 1), dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        for model in models:\n",
    "            outputs = model(theta_tensor)  # (n_candidates, 2)\n",
    "            mu = outputs[:, 0].numpy()\n",
    "            log_sigma = outputs[:, 1].numpy()\n",
    "            sigma = np.exp(log_sigma) + sigma_min\n",
    "            # Compute the likelihood density using the Gaussian PDF\n",
    "            L = (1.0/np.sqrt(2*np.pi*sigma**2)) * np.exp(-0.5*((x0 - mu)**2)/(sigma**2))\n",
    "            all_preds.append(L)\n",
    "        # Shape: (n_models, n_candidates) --> transpose to (n_candidates, n_models)\n",
    "        all_preds = np.array(all_preds).T\n",
    "    return all_preds\n",
    "\n",
    "# --- Acquisition Rule (MaxVar) -----------------------------------------------\n",
    "def acquisition_maxvar_ensemble(models, grid, x0=2):\n",
    "    \"\"\"\n",
    "    Evaluate the ensemble synthetic likelihood on a grid of theta values,\n",
    "    and return the theta corresponding to the maximum sample variance across ensemble predictions.\n",
    "    \"\"\"\n",
    "    preds = ensemble_predict(models, grid, x0)\n",
    "    # For each candidate theta, compute the sample variance across ensemble members\n",
    "    var_preds = np.var(preds, axis=1)\n",
    "    idx = np.argmax(var_preds)\n",
    "    return grid[idx]\n",
    "\n",
    "def acquisition_uniform_ensemble():\n",
    "    \"\"\"Uniform acquisition: return a random theta from the prior.\"\"\"\n",
    "    return np.random.uniform(-8, 8)\n",
    "\n",
    "# --- Main Simulation Loop using the Neural Network Ensemble -----------------\n",
    "def run_simulation_ensemble(acquisition_rule='maxvar', N_initial=10, N_acq=100, x0=2, sigma=0.31622777,\n",
    "                            ensemble_size=10, epochs_per_round=200):\n",
    "    \"\"\"\n",
    "    Run one simulation run using a neural network ensemble.\n",
    "      - acquisition_rule: either 'uniform' or 'maxvar'\n",
    "      - N_initial: initial sample count\n",
    "      - N_acq: number of acquisition rounds\n",
    "      - ensemble_size: number of ensemble members\n",
    "      - epochs_per_round: training epochs for each acquisition round\n",
    "    Returns:\n",
    "      tv_errors: list of total variation distances (one per acquisition round)\n",
    "      data: (theta, x) arrays for inspection if needed.\n",
    "    \"\"\"\n",
    "    # --- Initialize the training data -------------------------------\n",
    "    thetas = np.random.uniform(-8, 8, size=N_initial)\n",
    "    x_samples = np.array([simulator_1d(t, sigma) for t in thetas])\n",
    "    # Convert to torch tensors (shape: (N, 1))\n",
    "    theta_tensor = torch.tensor(thetas.reshape(-1, 1), dtype=torch.float32)\n",
    "    x_tensor = torch.tensor(x_samples.reshape(-1, 1), dtype=torch.float32)\n",
    "    \n",
    "    # Initialize ensemble\n",
    "    models = [EmulatorNet() for _ in range(ensemble_size)]\n",
    "    optimizers = [optim.Adam(model.parameters(), lr=0.01) for model in models]\n",
    "    \n",
    "    # Set up evaluation grid over theta for posterior reconstruction\n",
    "    grid = np.linspace(-8, 8, 400)\n",
    "    true_post = true_posterior_1d(grid, x0, sigma)\n",
    "    \n",
    "    tv_errors = []\n",
    "    \n",
    "    for acq in trange(N_acq):\n",
    "        # Train the ensemble on the current dataset\n",
    "        train_ensemble(models, optimizers, theta_tensor, x_tensor, epochs=epochs_per_round)\n",
    "        \n",
    "        # Predict synthetic likelihood on the grid\n",
    "        preds = ensemble_predict(models, grid, x0)  # shape (len(grid), ensemble_size)\n",
    "        # Average across the ensemble to get synthetic likelihood estimate\n",
    "        L_est = np.mean(preds, axis=1)\n",
    "        # Obtain synthetic posterior (proportional to L_est, uniform prior)\n",
    "        post_est = L_est / (np.trapz(L_est, grid) + 1e-10)\n",
    "        # Compute TV distance between synthetic posterior and true posterior\n",
    "        dtheta = grid[1] - grid[0]\n",
    "        tv = 0.5 * np.sum(np.abs(post_est - true_post)) * dtheta\n",
    "        tv_errors.append(tv)\n",
    "        \n",
    "        # Acquisition: choose new theta according to rule\n",
    "        if acquisition_rule == 'uniform':\n",
    "            new_theta = acquisition_uniform_ensemble()\n",
    "        elif acquisition_rule == 'maxvar':\n",
    "            new_theta = acquisition_maxvar_ensemble(models, grid, x0)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown acquisition rule: choose 'uniform' or 'maxvar'\")\n",
    "        \n",
    "        # Simulate a new observation at the new theta\n",
    "        new_x = simulator_1d(new_theta, sigma)\n",
    "        # Append new data to the training set\n",
    "        thetas = np.append(thetas, new_theta)\n",
    "        x_samples = np.append(x_samples, new_x)\n",
    "        theta_tensor = torch.tensor(thetas.reshape(-1, 1), dtype=torch.float32)\n",
    "        x_tensor = torch.tensor(x_samples.reshape(-1, 1), dtype=torch.float32)\n",
    "        \n",
    "    return tv_errors, (thetas, x_samples)\n",
    "\n",
    "# --- Evaluate Ensemble Inference over Multiple Runs --------------------------\n",
    "def evaluate_ensemble(N_runs=5, N_initial=10, N_acq=100, rule='maxvar', ensemble_size=10, epochs_per_round=200):\n",
    "    all_tv = []\n",
    "    for run in range(N_runs):\n",
    "        print(f'run: {run}')\n",
    "        tv, _ = run_simulation_ensemble(acquisition_rule=rule, N_initial=N_initial, N_acq=N_acq,\n",
    "                                        ensemble_size=ensemble_size, epochs_per_round=epochs_per_round)\n",
    "        all_tv.append(tv)\n",
    "    return np.array(all_tv)\n",
    "\n",
    "# Run ensemble simulations with maxvar and uniform acquisitions\n",
    "print('---- max var ----')\n",
    "tv_maxvar_ensemble = evaluate_ensemble(rule='maxvar')\n",
    "print('---- ensemble ----')\n",
    "tv_uniform_ensemble = evaluate_ensemble(rule='uniform')\n",
    "\n",
    "acq_steps = np.arange(1, 101)\n",
    "mean_maxvar = tv_maxvar_ensemble.mean(axis=0)\n",
    "mean_uniform = tv_uniform_ensemble.mean(axis=0)\n",
    "sem_maxvar = tv_maxvar_ensemble.std(axis=0) / np.sqrt(tv_maxvar_ensemble.shape[0])\n",
    "sem_uniform = tv_uniform_ensemble.std(axis=0) / np.sqrt(tv_uniform_ensemble.shape[0])\n",
    "\n",
    "# --- Plotting the Results -----------------------------------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "# (a) Plot the simulator function f(theta) and the observed x0\n",
    "theta_grid = np.linspace(-8, 8, 400)\n",
    "plt.plot(theta_grid, f_1d(theta_grid), label='f(theta)')\n",
    "plt.axhline(2, color='r', linestyle='--', label='x0 = 2')\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('f(theta)')\n",
    "plt.title('Simulator Function')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.errorbar(acq_steps, mean_uniform, yerr=sem_uniform, label='Uniform Acq', capsize=3)\n",
    "plt.errorbar(acq_steps, mean_maxvar, yerr=sem_maxvar, label='MaxVar Acq', capsize=3)\n",
    "plt.xlabel('Acquisitions')\n",
    "plt.ylabel('TV Distance')\n",
    "plt.title('Ensemble-Based Inference (1D)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4beba9-55e4-40bd-92b4-f38c68408d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "sbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
